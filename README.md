[ç®€ä½“ä¸­æ–‡](README_ZN.md) | English

# Autonomous Driving Perception Module

A **multi-task learning** project for autonomous driving perception, featuring **lane line detection** and **traffic sign recognition** using a shared encoder architecture.

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![ROS2](https://img.shields.io/badge/ROS2-Humble-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## ğŸŒŸ Features

- **Multi-Task UNet Architecture**: Shared encoder with separate decoder heads for lane segmentation and traffic sign classification
- **Joint Training**: Simultaneous optimization of both tasks with configurable loss weighting
- **ONNX Export**: Production-ready model export for deployment
- **ROS2 Integration**: Real-time perception node for autonomous driving applications
- **Containerized**: Full Docker support for training and inference
- **Comprehensive Testing**: pytest-based test suite

## ğŸ—ï¸ Architecture Overview

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Input Image    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Shared Encoder  â”‚
                    â”‚  (CNN Backbone)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                             â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Segmentation     â”‚         â”‚ Classification   â”‚
     â”‚ Decoder (UNet)   â”‚         â”‚ Head (FC)        â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                             â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Lane Mask       â”‚         â”‚  Sign Class      â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Requirements

- Python 3.10+
- PyTorch 2.0+
- CUDA (optional, for GPU training)
- ROS2 Humble (for ROS2 integration)

## ğŸš€ Installation

### Option 1: Python Virtual Environment

```bash
# Clone the repository
git clone https://github.com/yourusername/Autonomous-Driving-Perception-Module.git
cd Autonomous-Driving-Perception-Module

# Create and activate virtual environment
python -m venv venv
# On Windows:
venv\Scripts\activate
# On Linux/Mac:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Option 2: Docker

```bash
# Build the Docker image
docker build -t adpm:latest .

# Run with GPU support
docker run --gpus all -it -v $(pwd):/workspace adpm:latest

# Run without GPU
docker run -it -v $(pwd):/workspace adpm:latest
```

## âš¡ Quickstart

### 1. Prepare Configuration

Edit the configuration file to set your dataset paths:

```bash
cp configs/config_example.yaml configs/my_config.yaml
# Edit configs/my_config.yaml with your dataset paths
```

### 2. Generate Dummy Data (Optional)

For testing the pipeline without real data:

```bash
python scripts/prepare_dummy_data.py
```

### 3. Train the Model

```bash
python -m src.training.train --config configs/config_example.yaml
```

### 4. Export to ONNX

```bash
python -m src.inference.export_onnx --config configs/config_example.yaml --checkpoint checkpoints/best_model.pth
```

### 5. Run ONNX Inference

```bash
python -m src.inference.infer_onnx --config configs/config_example.yaml --image path/to/image.jpg --output output/
```

## ğŸ¤– ROS2 Integration

### Prerequisites

- ROS2 Humble or Foxy installed
- ONNX model exported

### Running the ROS2 Node

```bash
# Source ROS2
source /opt/ros/humble/setup.bash

# Run the perception node
ros2 run src.ros2_integration lane_sign_node --ros-args \
    -p onnx_model_path:=outputs/model.onnx \
    -p config_path:=configs/config_example.yaml
```

### Using Launch File

```bash
ros2 launch src.ros2_integration lane_sign.launch.py \
    onnx_model_path:=outputs/model.onnx \
    config_path:=configs/config_example.yaml
```

### Topics

| Topic | Type | Description |
|-------|------|-------------|
| `/camera/image_raw` | `sensor_msgs/Image` | Input camera image |
| `/perception/lane_mask` | `sensor_msgs/Image` | Lane segmentation mask |
| `/perception/sign_label` | `std_msgs/String` | Recognized traffic sign |

## ğŸ§ª Testing

Run the test suite with pytest:

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_model_forward.py -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

## ğŸ“ Repository Structure

```
Autonomous-Driving-Perception-Module/
â”œâ”€â”€ README.md                   # English documentation
â”œâ”€â”€ README_ZN.md               # Chinese documentation
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ Dockerfile                 # Container configuration
â”œâ”€â”€ pyproject.toml            # Project metadata and tooling
â”œâ”€â”€ .gitignore                # Git ignore rules
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config_example.yaml   # Example configuration
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ technical_report.md   # Technical documentation
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset.py        # Dataset classes
â”‚   â”‚   â””â”€â”€ transforms.py     # Data augmentation
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ multitask_unet.py # Multi-task model
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train.py          # Training script
â”‚   â”‚   â”œâ”€â”€ losses.py         # Loss functions
â”‚   â”‚   â”œâ”€â”€ metrics.py        # Evaluation metrics
â”‚   â”‚   â””â”€â”€ utils.py          # Training utilities
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ export_onnx.py    # ONNX export
â”‚   â”‚   â””â”€â”€ infer_onnx.py     # ONNX inference
â”‚   â””â”€â”€ ros2_integration/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ lane_sign_node.py # ROS2 node
â”‚       â””â”€â”€ launch/
â”‚           â””â”€â”€ lane_sign.launch.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_dataset.py
â”‚   â”œâ”€â”€ test_model_forward.py
â”‚   â”œâ”€â”€ test_onnx_export.py
â”‚   â””â”€â”€ test_ros2_node_import.py
â””â”€â”€ scripts/
    â”œâ”€â”€ prepare_dummy_data.py
    â”œâ”€â”€ run_training_example.sh
    â””â”€â”€ run_training_example.bat
```

## ğŸ“Š Supported Datasets

This project is designed to be dataset-agnostic. Example compatible datasets:

- **Lane Detection**: TuSimple, CULane, BDD100K
- **Traffic Sign Recognition**: GTSRB, TT100K

See the configuration file for dataset path setup.

## ğŸ“„ License

This project is open-source. Please choose an appropriate license (MIT, Apache 2.0, etc.) based on your needs.

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Code Style

- Follow PEP8 guidelines
- Write comments in Simplified Chinese
- Include type hints in function signatures
- Add tests for new features

## ğŸ“¬ Contact

For questions and support, please open an issue on GitHub.

---

**â­ Star this repository if you find it helpful!**
